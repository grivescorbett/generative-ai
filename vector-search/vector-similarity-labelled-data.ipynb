{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-dXXfV_pGa8"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv3oVInApMae"
      },
      "source": [
        "# LLM text embeddings visualization\n",
        "\n",
        "This notebook demonstrates how vector similarity is relevant to LLM-generated embeddings. It will embed a collection of labelled documents and then perform a clustering analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tdCmJnzpkmi"
      },
      "source": [
        "## Getting started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODKbeVNopmrM"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM6F3k69UdCE"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.0.315\n",
        "!pip install google-cloud-aiplatform==1.35.0\n",
        "!pip install scikit-learn==1.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s04fztzqYek"
      },
      "source": [
        "### Authenticating your notebook environment\n",
        "\n",
        "* If you are using **Colab** to run this notebook, uncomment the cell below and continue\n",
        "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4mvUNbzqZH8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth as google_auth\n",
        "# google_auth.authenticate_user()\n",
        "\n",
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYN5w2llqRiN"
      },
      "source": [
        "### Import libraries\n",
        "\n",
        "***Colab only***: Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niV1MdReqdsq"
      },
      "outputs": [],
      "source": [
        "# from google.cloud import aiplatform\n",
        "# PROJECT_ID = '[YOUR PROJECT ID]'\n",
        "# aiplatform.init(project=PROJECT_ID, location='us-central1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMZ9npzcVDn6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.api_core import retry\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmck9vqZqj-U"
      },
      "source": [
        "## Fetch and clean the data\n",
        "\n",
        "In this example we are using the open source [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset, a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgF9FsB5aTc9"
      },
      "outputs": [],
      "source": [
        "categories = [\"comp.graphics\", \"sci.space\", \"sci.med\", \"rec.sport.hockey\"]\n",
        "newsgroups = fetch_20newsgroups(categories=categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuBYC5Q-ac7K"
      },
      "outputs": [],
      "source": [
        "raw_data = pd.DataFrame()\n",
        "raw_data[\"text\"] = newsgroups.data\n",
        "raw_data[\"target\"] = [newsgroups.target_names[x] for x in newsgroups.target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CexQC3-0rEsg"
      },
      "source": [
        "Because of the 8k token limit, in this example we will exclude all documents less than 8,000 characters. Don't confuse tokens with characters, the number could be higher, this is just to be safe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBjP-_Voadij"
      },
      "outputs": [],
      "source": [
        "filtered = raw_data.loc[raw_data[\"text\"].str.len() <= 8000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t51GoVa4rMGM"
      },
      "source": [
        "Subsample the dataset into 500 data points, stratified on the label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDEzf8eucexS"
      },
      "outputs": [],
      "source": [
        "x_subsample, _, y_subsample, _ = train_test_split(\n",
        "    raw_data[\"text\"], raw_data[\"target\"], stratify=raw_data[\"target\"], train_size=500\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAYckCCcrU8Z"
      },
      "source": [
        "Clean out the text removing emails, names, etc. Even with Gen AI, garbage in means garbage out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7SMLgsTddIW"
      },
      "outputs": [],
      "source": [
        "x_subsample = [re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", d) for d in x_subsample]  # Remove email\n",
        "x_subsample = [re.sub(r\"\\([^()]*\\)\", \"\", d) for d in x_subsample]  # Remove names\n",
        "x_subsample = [d.replace(\"From: \", \"\") for d in x_subsample]  # Remove \"From: \"\n",
        "x_subsample = [\n",
        "    d.replace(\"\\nSubject: \", \"\") for d in x_subsample\n",
        "]  # Remove \"\\nSubject: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ak41eeiddyM"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"text\"] = x_subsample\n",
        "df[\"target\"] = list(y_subsample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "easflrmcrbFw"
      },
      "source": [
        "We now have 500 data points roughly evenly distributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEzHxV_9d4xl"
      },
      "outputs": [],
      "source": [
        "df[\"target\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VeSVs5hriMq"
      },
      "source": [
        "## Create and visualize the embeddings\n",
        "\n",
        "Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jte4vh6oeR9X"
      },
      "outputs": [],
      "source": [
        "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csrvfLqRe0qC"
      },
      "outputs": [],
      "source": [
        "# Retrieve embeddings from the specified model with retry logic\n",
        "def make_embed_text_fn(model):\n",
        "    @retry.Retry(timeout=300.0)\n",
        "    def embed_fn(text):\n",
        "        return model.get_embeddings([text])[0].values\n",
        "\n",
        "    return embed_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQX_nVIyrzCl"
      },
      "source": [
        "Create the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUySDL6ffQDt"
      },
      "outputs": [],
      "source": [
        "df[\"embeddings\"] = df[\"text\"].progress_apply(make_embed_text_fn(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkKoUkLuffsS"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrwmPDv4r2oc"
      },
      "source": [
        "The vectors generate by our model are 768 dimensions, we're not able to visualize them in their raw form. Use [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to reduce to 2 dimensions for us humans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzMvneXyhQcO"
      },
      "outputs": [],
      "source": [
        "embeddings_array = np.array(df[\"embeddings\"].to_list(), dtype=np.float32)\n",
        "tsne = TSNE(random_state=0, n_iter=1000)\n",
        "tsne_results = tsne.fit_transform(embeddings_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf6NxWXuhS3q"
      },
      "outputs": [],
      "source": [
        "df_tsne = pd.DataFrame(tsne_results, columns=[\"TSNE1\", \"TSNE2\"])\n",
        "df_tsne[\"target\"] = df[\"target\"]  # Add labels column from df_train to df_tsne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaGVCe64hhjU"
      },
      "outputs": [],
      "source": [
        "df_tsne.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8vlRIWdsFz0"
      },
      "source": [
        "Plot the data points. It's clear the documents from the same newsgroup embed closely to eachother in the vector space. This will be useful in the next lab when we use embeddings to find similar documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPgAMTklhkXV"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))  # Set figsize\n",
        "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
        "sns.scatterplot(data=df_tsne, x=\"TSNE1\", y=\"TSNE2\", hue=\"target\", palette=\"hls\")\n",
        "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
        "plt.title(\"Scatter plot of news using t-SNE\")\n",
        "plt.xlabel(\"TSNE1\")\n",
        "plt.ylabel(\"TSNE2\")\n",
        "plt.axis(\"equal\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "RAG_L300_Part_1_Vector_Similarity.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
